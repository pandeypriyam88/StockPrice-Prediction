import yfinance
import pandas as pd
import numpy as np
import math
import pandas_datareader as web
import matplotlib.pyplot as plt 
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense 

stocks= web.DataReader('AAPL',data_source='yahoo', start= '2020-01-01', end = datetime.datetime.now() )
print(stocks)

plt.figure(figsize=(16,8))
plt.title('CLOSE PRICE HISTORY')
plt.plot(stocks['Close'])
plt.xlabel('date', fontsize=18)
plt.ylabel('CLOSE Price', fontsize=18)
plt.show()

#creating a dataframe with only close coulmn

data = stocks.filter(['Close'])
print(data)
#convert thedataframes into numpy array
dataset= data.values
print(dataset)

training_data_len= math.ceil(len(dataset)*.8)
#i want to train the model upto 80% on the data we have so I'll multiply dataset with 0.8%
print("lenght of dataset",training_data_len)

#scale the data
scaler= MinMaxScaler(feature_range=(0,1))
print(scaler)
scaled_data= scaler.fit_transform(dataset)
print(scaled_data)


train_data = scaled_data[0:training_data_len, :]
x_train=[]
y_train=[]


for i in range(60, len(train_data)):
    x_train.append(train_data[i-60:i,0])
    y_train.append(train_data[i, 0])
    if i<=61:
        print(x_train)
        print(y_train)
        print()
        
#converting the x_train & y_train to numpy arrays
x_train=np.array(x_train)
y_train=np.array(y_train)


#reshaping the data
#why do we reshape the data? a lstm network expects the model data into 3-d  in no-of time stamps,no of samples &
#no of features 
x_train= np.reshape(x_train,(x_train.shape[0], x_train.shape[1], 1))
print(x_train)

#build the LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(x_train.shape[1],1)))
model.add(LSTM(50, return_sequences=False))
model.add(Dense(25))
model.add(Dense(1))

#compiling the model
model.compile(optimizer='adam',loss='mean_squared_error')

#train the model
model.fit(x_train, y_train, batch_size=1, epochs=1)
#creating the testing dataset
#to create the testing dataset lets take the vakues fromindex 1543 to 2003
test_data= scaled_data[training_data_len - 60: , :]
#creating the x_test & y_test dataset for testing datasets
x_test=[]
y_test=dataset[training_data_len:, :]
for i in range(60,len(test_data)):
    x_test.append(test_data[i-60:i, 0])
    
#converting the data into numpy array
x_test=np.array(x_test)

#converting the data into numpy array
x_test=np.array(x_test)

x_test= np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))
print(x_test)
#reshaping the testing dataset from 2d to 3d
#get the predicted price value
predictions = model.predict(x_test)
predictions = scaler.inverse_transform(predictions)
#get the root mean square
rmse= np.sqrt(np.mean(predictions-y_test)**2)
print(rmse)
#plot the data
train = data[:training_data_len]
valid = data[training_data_len:]
valid['predictions'] = predictions
#visulising the data
plt.figure(figsize=(16,8))
plt.title('Model')
plt.xlabel('Date',fontsize=18)
plt.ylabel('Close Price USD($)',fontsize=18)
plt.plot(train['Close'])
plt.plot(valid[['Close','predictions']])
plt.legend(['train','Val','predictions'], loc='lower right')
plt.show()
print(valid)
#get the quote
tesla_quote= web.DataReader('AAPL',data_source='yahoo', start= '2020-01-01', end = datetime.datetime.now() )
print(tesla_quote)
#create new dataframe
new_df=tesla_quote.filter(['Close'])
print(new_df)
#get the last 60 days closing price and converting the dataframes to array
last_60_days=new_df[-60:].values
#scale the data to be  values between 0 & 1
last_60_days_scaled=scaler.transform(last_60_days)
#create an empty list
Xtest=[]
#append the last 60days
Xtest.append(last_60_days_scaled)
Xtest=np.array(Xtest)
Xtest=np.reshape(Xtest, (Xtest.shape[0],Xtest.shape[1],1))
pred_price=model.predict(Xtest)
pred_price=scaler.inverse_transform(pred_price)
print(pred_price)
#get the quote
tesla_quote2= web.DataReader('AAPL', data_source='yahoo', start= '2020-06-24', end = '2020-06-25' )
print(tesla_quote2['Close'])
